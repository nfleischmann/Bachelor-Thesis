{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RFwGcO_dzfR9",
        "4YRp5Zalz1qB",
        "BhXIQR0_f9GY",
        "nIDb6FJQ2YEa",
        "oYiKLbwyUGHj",
        "MW96ngW3nHF5",
        "PRPlUorOqPgt"
      ],
      "authorship_tag": "ABX9TyMty6We+CnOUL3VgQxe9oha",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nfleischmann/Gradient-Based-Methods-for-the-Training-of-Neural-Networks/blob/main/experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg3DoxpJpSoL"
      },
      "source": [
        "# 5.1 Emprical Analysis of Local Minima\n",
        "\n",
        "In this experiment, we will empirically investigate the relationship between the local minima of the objective function and the number of hidden neurons of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvWiYvWkb8kS"
      },
      "source": [
        "# Load libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFwGcO_dzfR9"
      },
      "source": [
        "## Training Set\n",
        "\n",
        "We use a small training set generated by the function make\\_circles of the library sci-kit learn throughout the experiment. It consists of 100 observations that have a real input $x \\in \\mathbb{R}^2$ and a binary output $y \\in \\{0,1\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpnPHxJQc3RQ"
      },
      "source": [
        "# Generate 10000 observations according to the function make_circles\n",
        "X,y = make_circles(n_samples=10000, shuffle=False, random_state=1, factor=0, noise=0.35)\n",
        "\n",
        "# Split the data in training (n = 100) and test set (n = 9900)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=1)\n",
        "\n",
        "# Compute the training set size, the input shape and the test set size\n",
        "n = X_train.shape[0]\n",
        "d_0 = X_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YRp5Zalz1qB"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "We consider neural networks with two input neurons, $L-1 \\in \\{1,2,3\\}$ hidden layers, and one output neuron. Thereby all hidden layers contain the same number of neurons $d_i \\in \\{$1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 75, 100$\\}$. For the hidden neurons, we use the leaky ReLU, whereas the logistic function is employed as the activation function for the output neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w50_iM-Adc3R"
      },
      "source": [
        "def build_model(n_hidden, width, input_shape=[2], initializer_range=100, seed=42):\n",
        "  \"\"\" Builds a neural network model\n",
        "\n",
        "    Args:\n",
        "        n_hidden (int): Number of hidden layers\n",
        "        width (int): Width of the hidden layers\n",
        "        input_shape (list): Shape of the input layer\n",
        "          (default is [2])\n",
        "        initializer_range(int): Range of the Uniform distribution used to initialize\n",
        "                                the parameters\n",
        "          (default is 100)\n",
        "        seed (int): Seed that makes the initialization of the model reproducible\n",
        "          (default is 42)\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.model: Initialized neural network model\n",
        "  \"\"\"\n",
        "  # Define initializer\n",
        "  initializer = tf.keras.initializers.RandomUniform(minval=-initializer_range, maxval=initializer_range) \n",
        "\n",
        "  # Initialize neural network model\n",
        "  tf.random.set_seed(seed)\n",
        "  model = keras.models.Sequential() \n",
        "\n",
        "  # Add input layer\n",
        "  model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "  # Add hidden layers\n",
        "  for layer in range(n_hidden): \n",
        "    model.add(keras.layers.Dense(width, \n",
        "                                 activation=keras.layers.LeakyReLU(alpha=0.1), \n",
        "                                 kernel_initializer=initializer, \n",
        "                                 bias_initializer=initializer))\n",
        "\n",
        "  # Add output layer\n",
        "  model.add(keras.layers.Dense(1,\n",
        "                               activation=\"sigmoid\", \n",
        "                               kernel_initializer=initializer, \n",
        "                               bias_initializer=initializer))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhXIQR0_f9GY"
      },
      "source": [
        "## Experimental Design\n",
        "\n",
        "We search for local minima of the respective objective function for each neural network and compute their cost. We use gradient descent to determine the local minima. We let gradient descent make 40,000 steps with a fixed step size and then adaptively lower the step size (after 50 steps without descent, the step size is halved). The method is stopped after 10,000 steps in which the cost improved less than 1e-7 or at the latest after 120,000 steps. In order to find different local minima, we run gradient descent several times. For each run, all parameters of the neural networks are initialized according to a continuous uniform distribution. This way, gradient descent is started 50 times for the neural networks with one hidden layer and 15 times for those with 2 or 3 hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDb6FJQ2YEa"
      },
      "source": [
        "### Neural Networks with one Hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JOF986gAfh6Z"
      },
      "source": [
        "n_hidden = 1\n",
        "\n",
        "# Define the widths for which we will execute this experiment\n",
        "widths = [1,2,3,4,5,10,15,20,30,40,50,75,100]\n",
        "# Define seeds used for initializing the neural networks\n",
        "seeds = list(range(50))\n",
        "\n",
        "# Define a callback that stops gradient descent after 10.000 iterations without\n",
        "# significant progress\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                              min_delta=0.0000001, \n",
        "                                              patience=10000)\n",
        "\n",
        "# Define a callback that adaptively halves the learning rate after 50 iterations \n",
        "# without progress\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=40,\n",
        "                                                 cooldown=10,\n",
        "                                                 min_lr=0.0000001)\n",
        "\n",
        "# Initialize the dataframe in which we will store the results\n",
        "results_1_hidden = pd.DataFrame(columns=['n_hidden', 'width', 'cost'])\n",
        "idx = 0 \n",
        "\n",
        "for seed in seeds:\n",
        "  for width in widths:\n",
        "    # Build neural network with 1 hidden layer and 'width' hidden neurons\n",
        "    model = build_model(n_hidden, width, seed=seed)\n",
        "\n",
        "    # Perform (at most) 40.000 gradient descent steps with a constant step size\n",
        "    model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(learning_rate=1))\n",
        "    model.fit(X_train, y_train, \n",
        "              batch_size=n, \n",
        "              epochs=40000,\n",
        "              callbacks=[early_stop])\n",
        "\n",
        "    # Perform (at most) 80.000 gradient descent steps with decreasing step sizes\n",
        "    history = model.fit(X_train, y_train, \n",
        "                        batch_size=n, \n",
        "                        epochs=80000,\n",
        "                        callbacks=[early_stop, reduce_lr])\n",
        "\n",
        "    results_1_hidden.loc[idx, 'n_hidden'] = 1\n",
        "    results_1_hidden.loc[idx, 'width'] = width\n",
        "    results_1_hidden.loc[idx, 'cost'] = history.history['loss'][-1]\n",
        "    idx = idx + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYiKLbwyUGHj"
      },
      "source": [
        "### Neural Networks with Two or Three Hidden Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PNXsVQgUFYd"
      },
      "source": [
        "hidden_layers = [2,3]\n",
        "\n",
        "# Define the widths for which we will execute this experiment\n",
        "widths = [1,2,3,4,5,10,15,20,30,40,50,75,100]\n",
        "\n",
        "# Define seeds used for initializing the neural networks\n",
        "seeds = list(range(15))\n",
        "\n",
        "# Define a callback that stops gradient descent after 10.000 iterations without\n",
        "# significant progress\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                              min_delta=0.0000001, \n",
        "                                              patience=10000)\n",
        "\n",
        "# Define a callback that adaptively halves the learning rate after 50 iterations \n",
        "# without progress\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=40,\n",
        "                                                 cooldown=10,\n",
        "                                                 min_lr=0.0000001)\n",
        "\n",
        "# Initialize the dataframe in which we will store the results\n",
        "results_2_3_hidden = pd.DataFrame(columns=['n_hidden', 'width', 'cost'])\n",
        "idx = 0 \n",
        "\n",
        "for n_hidden in hidden_layers:\n",
        "  # Neural Networks with more hidden layers are more sensitive to the step size\n",
        "  # therefore we choose different step sizes for the different networks\n",
        "  if n_hidden == 2:\n",
        "    lr = 0.5\n",
        "  if n_hidden == 3:\n",
        "    lr = 0.1\n",
        "\n",
        "  for seed in seeds:\n",
        "    for width in widths:\n",
        "\n",
        "      # Build neural network\n",
        "      model = build_model(n_hidden, width, initializer_range=1, seed=seed)\n",
        "\n",
        "      # Perform (at most) 40.000 gradient descent steps with constant a step size\n",
        "      model.compile(loss=\"binary_crossentropy\",\n",
        "                    optimizer=keras.optimizers.SGD(learning_rate=lr))\n",
        "      model.fit(X_train, y_train, \n",
        "                batch_size=n, \n",
        "                epochs=40000,\n",
        "                callbacks=[early_stop])\n",
        "\n",
        "      # Perform (at most) 80.000 gradient descent steps with decreasing step sizes\n",
        "      history = model.fit(X_train, y_train, \n",
        "                          batch_size=n, \n",
        "                          epochs=80000,\n",
        "                          callbacks=[early_stop, reduce_lr])\n",
        "\n",
        "      # Fill results dataframe\n",
        "      results_2_3_hidden.loc[idx, 'n_hidden'] = n_hidden\n",
        "      results_2_3_hidden.loc[idx, 'width'] = width\n",
        "      results_2_3_hidden.loc[idx, 'cost'] = history.history['loss'][-1]\n",
        "      idx = idx + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JTqVeSmJ4t"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2VVLvdKocLC"
      },
      "source": [
        "# Import libraries\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW96ngW3nHF5"
      },
      "source": [
        "### Figure 5.1\n",
        "\n",
        "Depicts the training set and its underlying distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPSsn_WBnM-4"
      },
      "source": [
        "X_plot,y_plot = make_circles(n_samples=2000000, shuffle=False, random_state=1, factor=0, noise = 0.35)\n",
        "Xy_df = pd.DataFrame({'x1':X[:,0],'x2':X[:,1],'y': y})\n",
        "\n",
        "# Seperate the data in the two classes\n",
        "y_1 = Xy_df.loc[Xy_df['y'] == 1]\n",
        "y_0 = Xy_df.loc[Xy_df['y'] == 0]\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "# Plot the estimated density of the class y = 1\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "kde = sns.kdeplot(x=y_1.x1, y=y_1.x2, cmap=\"Reds\", shade=True, bw_adjust=1)\n",
        "kde.set_xlabel(\"$x_1$\", fontsize=30)\n",
        "kde.set_ylabel(\"$x_2$\", fontsize=30)\n",
        "kde.tick_params(labelsize=20)\n",
        "ax.set(xlim=(-2,2),\n",
        "       ylim=(-2,2))\n",
        "plt.show()\n",
        "\n",
        "# Plot the estimated density of the class y = 0\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "kde = sns.kdeplot(x=y_0.x1, y=y_0.x2, cmap=\"Blues\", shade=True, bw_adjust=1)\n",
        "kde.set_xlabel(\"$x_1$\", fontsize=30)\n",
        "kde.set_ylabel(\"$x_2$\", fontsize=30)\n",
        "kde.tick_params(labelsize=20)\n",
        "ax.set(xlim=(-2,2),\n",
        "      ylim=(-2,2))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8tPDis9mMnr"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, s=120, edgecolors='k')\n",
        "ax.set_xlabel('$x_1$', fontsize=30)\n",
        "ax.set_ylabel('$x_2$', fontsize=30)\n",
        "plt.xlim((-2,2))\n",
        "plt.ylim((-2,2))\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.legend(*scatter.legend_elements(), title=\"y\", fontsize=30, title_fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRPlUorOqPgt"
      },
      "source": [
        "### Figure 5.2\n",
        "\n",
        "Depicts prediction functions that correspond to some of the found local minima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNBHc1fGqU8l"
      },
      "source": [
        "def plot_predictions(model, X_train, y_train):\n",
        "  \"\"\" Plot the predictions of the model along with the training set\n",
        "\n",
        "    Args:\n",
        "        model : Keras model\n",
        "        X_train (np.ndarray): Inputs of the training set\n",
        "        y_train (np.ndarray): Outputs of the training set\n",
        "  \"\"\"\n",
        "  # Define the grid\n",
        "  x_1 = x_2 = np.arange(-2, 2.1, 0.025)\n",
        "  x_len = np.size(x_1)\n",
        "  X_1, X_2 = np.meshgrid(x_1, x_2)\n",
        "\n",
        "  # Bring grid in the right format to make predictions\n",
        "  x_1 =  np.reshape(X_1, -1)\n",
        "  x_2 = np.reshape(X_2, -1)\n",
        "  X_12 = np.zeros((x_len**2, 2))\n",
        "  for i in range(x_len**2):\n",
        "    X_12[i] = np.array([x_1[i], x_2[i]])\n",
        "  \n",
        "  # Make Predictions for every element of the grid\n",
        "  z = model.predict(X_12)\n",
        "  Z = np.reshape(z,(x_len, x_len))\n",
        "\n",
        "  # Plot the results\n",
        "  fig, ax = plt.subplots(figsize=(12,12))\n",
        "  pred = ax.contourf(X_1, X_2, Z, cmap=plt.cm.RdBu_r, alpha=0.6,vmax=1, vmin=0)\n",
        "  scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, s=120, edgecolors='k')\n",
        "  ax.set_xlabel('$x_1$', fontsize=30)\n",
        "  ax.set_ylabel('$x_2$', fontsize=30)\n",
        "  plt.xticks(fontsize=20)\n",
        "  plt.yticks(fontsize=20)\n",
        "  plt.xlim((-2,2))\n",
        "  plt.ylim((-2,2))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1zrNFWesgfm"
      },
      "source": [
        "n_hidden = 1\n",
        "width_seeds = [[1,4],[3,29],[5,44],[10,50],[20,42],[50,31]]\n",
        "\n",
        "# Define a callback that stops gradient descent after 10.000 iterations without\n",
        "# significant progress\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                              min_delta=0.0000001, \n",
        "                                              patience=10000)\n",
        "\n",
        "# Define a callback that adaptively halves the learning rate after 50 iterations \n",
        "# without progress\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=40,\n",
        "                                                 cooldown=10,\n",
        "                                                 min_lr=0.0000001)\n",
        "\n",
        "for width_seed in width_seeds:\n",
        "  width = width_seed[0]\n",
        "  seed = width_seed[1]\n",
        "  \n",
        "  # Initialize neural network according to the seed\n",
        "  model = build_model(n_hidden, width, seed=seed)\n",
        "\n",
        "  # Optimize the neural network\n",
        "  model.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=keras.optimizers.SGD(learning_rate=1))\n",
        "  model.fit(X_train, y_train, \n",
        "            batch_size=n, \n",
        "            epochs=40000,\n",
        "            callbacks=[early_stop])\n",
        "  model.fit(X_train, y_train, \n",
        "            batch_size=n, \n",
        "            epochs=80000,\n",
        "            callbacks=[early_stop, reduce_lr])\n",
        "\n",
        "  # Plot the predictions\n",
        "  plot_predictions(model, X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}