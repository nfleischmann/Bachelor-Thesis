{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiedvTXKWxCjXlqSUIWdrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nfleischmann/Gradient-Based-Methods-for-the-Training-of-Neural-Networks/blob/main/experiment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsE1qjww8C0F"
      },
      "source": [
        "# Asymptotic Properties of Stochastic Gradient Methods\n",
        "\n",
        "One of the central results for stochastic gradient methods was that for a sufficiently small step size, the expected average squared norm of the gradients corresponding to the first $K$ iterates of a stochastic gradient method can be bounded as follows:\n",
        "$$\\mathbb{E} \\left[\\frac{1}{K}\\sum_{k = 1}^K \\lVert\\nabla f(\\theta_k)\\rVert_2^2\\right] \\leq \\; \\alpha L M + \\frac{2(f(\\theta_1) - f_{inf})}{K \\alpha}$$\n",
        "For large $K$, the upper bound is proportional to the step size $\\alpha$, and the noise of the stochastic vectors embodied by the term $M$. This relationship suggests that we can lower the expected average squared norm of the gradients by decreasing the step size and the noise term. In this experiment, we examine this relationship empirically for the training of neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVRrTE8O67E-"
      },
      "source": [
        "# Load libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPvy3PUP_p71"
      },
      "source": [
        "## Training Set\n",
        "\n",
        "As the training set for the experiment, we use a downscaled and downsampled version of the famous MNIST dataset. Each input $x \\in \\mathbb{R}^{14 \\times 14}$ is a $14 \\times 14$ greyscale image that shows either the digit three or six. Respectively the output $y \\in \\{0,1\\}$ takes on the value 0 for the digit three and 1 for the digit six. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuA2dTIO5plX"
      },
      "source": [
        "# Load the mnist data set from the keras API\n",
        "mnist = keras.datasets.mnist\n",
        "(X, y), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Subset the Classes 3 and 6\n",
        "X = X[(y == 3) | (y == 6)]\n",
        "y = y[(y == 3) | (y == 6)]\n",
        "\n",
        "# Normalize the brightness of the inputs to the range [0,1]\n",
        "X = X[..., np.newaxis]/255.0\n",
        "\n",
        "# Scale down the pictures from 28x28 to 14x14 to reduce the computiational cost\n",
        "X = tf.image.resize(X, (14,14)).numpy()\n",
        "X = np.squeeze(X, axis=3)\n",
        "\n",
        "# Change the labels to 0 for the digit 3 and 1 for the digit 6\n",
        "y[y == 3] = 0\n",
        "y[y == 6] = 1\n",
        "\n",
        "# Transform the training set into the right format\n",
        "y = np.asmatrix(y).transpose()\n",
        "X = np.reshape(X, (-1, 196))\n",
        "\n",
        "# Compute the training set size\n",
        "n = X.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMp9rojaAVJ8"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "We use a neural network with $14 \\times 14$ input neurons, three hidden layers with width ten, and a single output neuron. As the activation function for the hidden neurons, we use the ReLU and employ the logistic function for the output neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIDlfOO63ry"
      },
      "source": [
        "def build_model(input_shape=[14*14], n_hidden=3, width=10, seed=42):\n",
        "  \"\"\" Builds a neural network model\n",
        "\n",
        "    Args:\n",
        "        input_shape (list): Shape of the input layer\n",
        "          (default is [14*14])\n",
        "        n_hidden (int): Number of hidden layers\n",
        "          (default is 3)\n",
        "        width (int): Width of the hidden layers\n",
        "          (default is 10)\n",
        "        seed (int): Seed that makes the inialization of the model reproducible\n",
        "          (default is 42)\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.model: Initialized neural network model\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize Neural Network Model\n",
        "  tf.random.set_seed(seed)\n",
        "  model = keras.models.Sequential() \n",
        "\n",
        "  # Add input layer\n",
        "  model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "  # Add hidden layers\n",
        "  for layer in range(n_hidden): \n",
        "    model.add(keras.layers.Dense(width, activation=\"ReLU\"))\n",
        "\n",
        "  # Add output layer\n",
        "  model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV1TQswPbiSU"
      },
      "source": [
        "## Determine the Initial Iterate\n",
        "\n",
        "Since we are mainly interested in the asymptotic behavior, we want the second term in boundary $$\\mathbb{E} \\left[\\frac{1}{K}\\sum_{k = 1}^K \\lVert\\nabla f(\\theta_k)\\rVert_2^2\\right] \\leq \\; \\alpha L M + \\frac{2(f(\\theta_1) - f_{inf})}{K \\alpha}$$ to vanish. One way to achieve this is by choosing an initial value $\\theta_1$ for which the difference $f(\\theta_1) - f_{\\inf}$ is small. We determined such an initial value by applying gradient descent for 25,000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRWqI5qEbhwE"
      },
      "source": [
        "# Initialize neural network with 3 hidden layers with width 10\n",
        "model = build_model()\n",
        "\n",
        "# Perform 25.000 steps with gradient descent\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
        "model.fit(X, y, batch_size=n, epochs=25000)\n",
        "\n",
        "# Save the pretrained model\n",
        "model.save('./pretrained_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "licTHJCTeun-"
      },
      "source": [
        "## Stochastic and Mini-Batch Gradient Descent\n",
        "For this experiment we examine stochastic and mini-batch gradient descent. The standard implementation of Keras generates the samples for the stochastic gradient methods by shuffeling the training set and cycling through it (as we explained in Section 4.5.3). For this reason, we provide an alternative implementation that draws the mini-batch uniformly (with replacement) from the training set and also starts from the determined initial iterate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsMR9uttuBjI"
      },
      "source": [
        "def compute_squared_norm(grad):\n",
        "  \"\"\" Compute squared euclidean norm\n",
        "\n",
        "    Args:\n",
        "        grad (list): List that contains the derivatives of the trainable parameters of \n",
        "                     the neural network\n",
        "\n",
        "    Returns:\n",
        "        float: Compute squared euclidean norm of the gradient\n",
        "  \"\"\"\n",
        "  sum_of_squares = 0\n",
        "\n",
        "  for g in grad:\n",
        "      sum_of_squares = sum_of_squares + tf.math.reduce_sum(tf.square(g))\n",
        "      \n",
        "  return sum_of_squares.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRg2e6b0uNE2"
      },
      "source": [
        "def mini_batch_gradient_descent(X, y, step_size, mini_batch_size, steps, seed):\n",
        "  \"\"\" Implementation of mini-batch gradient descent\n",
        "\n",
        "    Args:\n",
        "        model: Keras model \n",
        "        X (np.ndarray): Inputs of the training set\n",
        "        y (np.ndarray): Outputs of the training set\n",
        "        step_size (int): Step size of the method\n",
        "        mini_batch_size (int): Size of the used mini-batch\n",
        "        steps (int): Number of steps for which mini-batch gradient descent is employed\n",
        "        seed (int): Seed used to make the drawn mini-batches reproduceable\n",
        "\n",
        "    Returns:\n",
        "        float: Average over the squared gradients of the objective function \n",
        "               corresponding to the first 'step' iterates\n",
        "  \"\"\"\n",
        "  # Load pretrained model \n",
        "  model = keras.models.load_model('./pretrained_model')\n",
        "\n",
        "  # Define the loss function and the optimizer(which determines the update rule)\n",
        "  log_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  optimizer = keras.optimizers.SGD(learning_rate=step_size)\n",
        "\n",
        "  # Determine the size of the training set\n",
        "  n = X.shape[0]\n",
        "\n",
        "  # Ensure that the samples for mini-batch gradient descent are repreducible\n",
        "  rng = np.random.default_rng(seed)\n",
        "\n",
        "  # Initialize the sum of the squared norm of the gradients\n",
        "  sum = 0\n",
        "\n",
        "  for step in range(steps):\n",
        "    # Sample the minibatch uniformly from the training set (with replacement)\n",
        "    mini_batch = rng.choice(n, mini_batch_size)\n",
        "    X_mini_batch = X[mini_batch]\n",
        "    y_mini_batch = y[mini_batch]\n",
        "            \n",
        "    # Compute the loss gradient for the mini-batch \n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = model(X_mini_batch, training=True)\n",
        "      loss = log_loss(y_mini_batch, pred)\n",
        "    grad = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    # Perform a step of mini-batch gradient descent\n",
        "    optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
        "\n",
        "    # Compute the gradient on the full training set / the gradient of the objective function\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred = model(X, training=True)\n",
        "      loss = log_loss(y, pred)\n",
        "    grad = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    # Compute the squared norm of the gradient and add it to the sum\n",
        "    sum = sum + compute_squared_norm(grad)\n",
        "  return sum/steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umk7xhuasLOn"
      },
      "source": [
        "## Impact of the Step Size\n",
        "\n",
        "To measure the impact of the step size, we computed the term $\\frac{1}{20} \\sum_{i = 1}^{20} \\left[\\frac{1}{30000}\\sum_{k = 1}^{30000} \\lVert\\nabla f(\\theta_k^{(i)})\\rVert_2^2\\right]$ multiple times for stochastic gradient descent. Each time we used a different step size $\\alpha \\in \\{$0.01, 0.009, 0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.002, 0.001$\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKLRtDojSR2l"
      },
      "source": [
        "step_sizes = [0.001,0.0009,0.0008,0.0007,0.0006,0.0005,0.0004,0.0003,0.0002,0.0001]\n",
        "seeds = list(range(20))\n",
        "\n",
        "metrics_step_size = pd.DataFrame(columns=['step_size','avg_squared_norm_gradients'])\n",
        "idx = 0 \n",
        "\n",
        "for seed in seeds:\n",
        "  rng = np.random.default_rng(seed)\n",
        "  for step_size in step_sizes: \n",
        "    metrics_step_size.loc[idx, 'step_size'] = step_size\n",
        "    metrics_step_size.loc[idx, 'avg_squared_norm_gradients'] = mini_batch_gradient_descent(X, y, \n",
        "                                                                                           step_size=step_size, \n",
        "                                                                                           mini_batch_size=1, \n",
        "                                                                                           steps=30000, \n",
        "                                                                                           seed=rng.choice(10000,1))\n",
        "    idx = idx + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI3370EZvbLv"
      },
      "source": [
        "## Impact of the Noise Term M\n",
        "\n",
        "In contrast to the step size, we can not adjust the noise term $M$ directly. Nevertheless, mini-batch gradient descent with mini-batch size $|B|$ reduces $M$ by a factor of $\\frac{1}{|B|}$ compared to stochastic gradient descent. Therefore we try to measure the impact of the noise term $M$ by computing the term $\\frac{1}{20} \\sum_{i = 1}^{20} \\left[\\frac{1}{30000}\\sum_{k = 1}^{30000} \\lVert\\nabla f(\\theta_k^{(i)})\\rVert_2^2\\right]$ for mini-batch gradient descent with different mini-batch sizes. To be more precise, we fix the step size $\\alpha =  0.001$ and employ the mini-batch sizes 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of3vLjW2fTDo"
      },
      "source": [
        "mini_batch_sizes = [1,2,3,4,5,6,7,8,9,10]\n",
        "seeds = list(range(20))\n",
        "\n",
        "metrics_mb_size = pd.DataFrame(columns=['mini_batch_size','avg_squared_norm_gradients'])\n",
        "idx = 0 \n",
        "\n",
        "for seed in seeds:\n",
        "  rng = np.random.default_rng(seed)\n",
        "  for mini_batch_size in mini_batch_sizes: \n",
        "    metrics_mb_size.loc[idx, 'mini_batch_size'] = mini_batch_size\n",
        "    metrics_mb_size.loc[idx, 'avg_squared_norm_gradients'] = mini_batch_gradient_descent(X, y, \n",
        "                                                                                         step_size=0.001, \n",
        "                                                                                         mini_batch_size=mini_batch_size, \n",
        "                                                                                         steps=30000, \n",
        "                                                                                         seed=rng.choice(10000,1))\n",
        "    idx = idx + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LZXc8FXNnW"
      },
      "source": [
        "## Figure 5.6\n",
        "\n",
        "Sample observations from the modified MNIST training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsFDDRXxXU_S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# Load the mnist data set from the keras API\n",
        "mnist = keras.datasets.mnist\n",
        "(X, y), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Subset the Classes 3 and 6\n",
        "X = X[(y == 3) | (y == 6)]\n",
        "y = y[(y == 3) | (y == 6)]\n",
        "\n",
        "# Normalize the brightness of the inputs to the range [0,1]\n",
        "X = X[..., np.newaxis]/255.0\n",
        "\n",
        "# Scale down the pictures from 28x28 to 14x14 to reduce the computiational cost\n",
        "X = tf.image.resize(X, (14,14)).numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "gs = GridSpec(nrows=2, ncols=2)\n",
        "ax0 = fig.add_subplot(gs[0, 0])\n",
        "ax0.imshow(X[0, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax1 = fig.add_subplot(gs[1, 0])\n",
        "ax1.imshow(X[2, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.imshow(X[42, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.imshow(X[10, :, :, 0], cmap=plt.cm.gray_r)\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "gs = GridSpec(nrows=2, ncols=2)\n",
        "ax0 = fig.add_subplot(gs[0, 0])\n",
        "ax0.imshow(X[4, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax1 = fig.add_subplot(gs[1, 0])\n",
        "ax1.imshow(X[9, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.imshow(X[66, :, :, 0], cmap=plt.cm.gray_r)\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.imshow(X[13, :, :, 0], cmap=plt.cm.gray_r)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}